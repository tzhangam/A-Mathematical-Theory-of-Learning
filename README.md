# A-Mathematical-Theory-of-Learning
---
##Dec 27 2017
* This repository is created for the research upon a mathematical understanding of learning. 
* The work here is motivated by the problem of underfitting and overfitting in Machine Learning industry.
* Using binary classification as an example, currently we say a ML model is underfitting if on a test set, it is not recognizing enough positive examples; we say a ML model is overfitting if on a test set, it is overkilling too many examples. 
* Underfitting is describing the phenomena that a ML model fails to grasp all the information provided during training, such that it is not able to complete the structure of the test set with satisfactory precision.
* Overfitting on the other hand, is describing the phenomena that a ML model is over biased on some features in the training set, such that it constructs a test set much too resembles the training set.
* Naturally for each ML model, there should be a critical point where the model is neither overfitting nor underfitting, that the model has learned just the right amount of knowledge from the training set, which is enough to reconstruct a test set and NOT biased towards some particular features.
---
* Then comes the big question: what do we mean by "learned just the right amount"? 
* **It is interesting to think about the definite measure of the quantity that a model learns.**
---
